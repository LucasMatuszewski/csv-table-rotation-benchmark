name: Performance Benchmarks

on:
  push:
    branches: [main]
    paths-ignore:
      - "**.md"
      - "LICENSE"
      - ".gitignore"
      - ".editorconfig"
      - ".vscode/**"
  pull_request:
    branches: [main]
    paths-ignore:
      - "**.md"
      - "LICENSE"
      - ".gitignore"
      - ".editorconfig"
      - ".vscode/**"
  schedule:
    # Run benchmarks weekly on Sunday at 02:00 UTC to detect performance regressions
    - cron: "0 2 * * 0"
  workflow_dispatch:
    # Allow manual triggering

jobs:
  benchmark:
    name: Cross-Language Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
      - uses: actions/checkout@v4

      # Setup all language environments
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: "stable"
          cache-dependency-path: go/go.sum

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "lts/*"
          cache: "npm"
          cache-dependency-path: typescript/package-lock.json

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Cache hyperfine
        id: cache-hyperfine
        uses: actions/cache@v4
        with:
          # Cache the hyperfine binary itself
          path: ~/.cargo/bin/hyperfine
          # Key is based on runner OS and a fixed version for stability
          key: ${{ runner.os }}-hyperfine-1.19.0

      - name: Install hyperfine
        # Install hyperfine using cargo if the cache was not restored
        if: steps.cache-hyperfine.outputs.cache-hit != 'true'
        run: cargo install hyperfine --version 1.19.0 --locked

      # Cache dependencies
      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            rust/target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Cache Go dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('go/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.11-${{ hashFiles('python/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-3.11-

      # Build all implementations
      - name: Build Rust (release mode)
        run: cargo build --release
        working-directory: rust

      - name: Build Go (release mode)
        run: go build -ldflags="-s -w" -o ./bin/rotate ./cmd/rotate
        working-directory: go

      - name: Build TypeScript
        run: |
          npm ci
          npm run build
        working-directory: typescript

      - name: Install Python package
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
        working-directory: python

      # Verify all binaries work
      - name: Verify implementations
        run: |
          echo "Testing Rust implementation..."
          ./rust/target/release/rotate_cli input-samples/sample-1k.csv | head -5

          echo "Testing Go implementation..."
          ./go/bin/rotate input-samples/sample-1k.csv | head -5

          echo "Testing TypeScript implementation..."
          node typescript/dist/index.js input-samples/sample-1k.csv | head -5

          echo "Testing Python implementation..."
          python -m rotate_cli input-samples/sample-1k.csv | head -5

      # Create benchmark results directory
      - name: Create benchmark results directory
        run: |
          mkdir -p benchmark-results
          date > benchmark-results/benchmark-info.txt
          echo "Runner: ${{ runner.os }}" >> benchmark-results/benchmark-info.txt
          echo "Commit: ${{ github.sha }}" >> benchmark-results/benchmark-info.txt
          echo "Ref: ${{ github.ref }}" >> benchmark-results/benchmark-info.txt
          echo "" >> benchmark-results/benchmark-info.txt
          echo "=== Language/Runtime Versions ===" >> benchmark-results/benchmark-info.txt
          echo "Rust: $(rustc --version)" >> benchmark-results/benchmark-info.txt
          echo "Go: $(go version)" >> benchmark-results/benchmark-info.txt
          echo "Node.js: $(node --version)" >> benchmark-results/benchmark-info.txt
          echo "Bun: $(bun --version)" >> benchmark-results/benchmark-info.txt
          echo "Python: $(python --version)" >> benchmark-results/benchmark-info.txt
          echo "Hyperfine: $(hyperfine --version)" >> benchmark-results/benchmark-info.txt

      # Run comprehensive benchmarks
      - name: Run comprehensive benchmarks
        run: |
          # Make script executable
          chmod +x ./benchmarks/run_hyperfine.sh

          # Run benchmarks with timestamp - disable cleanup so we can copy results
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          if ! DISABLE_CLEANUP=true ./benchmarks/run_hyperfine.sh 2>&1 | tee benchmark-results/benchmark-log-${TIMESTAMP}.txt; then
            echo "⚠️ Benchmark script encountered issues, but continuing with available results..."
            echo "script-failed=true" >> $GITHUB_OUTPUT
          fi

          # Copy all generated results
          if [ -d "./benchmarks/results" ]; then
            cp -r ./benchmarks/results/* benchmark-results/ || true
            echo "✅ Results copied from ./benchmarks/results/ to benchmark-results/"
          fi

          # Now clean up the original results directory to keep CI clean
          if [ -d "./benchmarks/results" ]; then
            find ./benchmarks/results -name "*.md" -delete 2>/dev/null || true
            find ./benchmarks/results -name "*.json" -delete 2>/dev/null || true
            find ./benchmarks/results -name "*.csv" -delete 2>/dev/null || true
            echo "✅ Original benchmark results cleaned up"
          fi

          # Debug: List what files were actually generated
          echo "Generated files in benchmarks/results:"
          find ./benchmarks/results -name "*.md" -o -name "*.json" -o -name "*.csv" | head -10 || echo "No files found"

          echo "Files copied to benchmark-results:"
          find benchmark-results -name "*.md" -o -name "*.json" -o -name "*.csv" | head -10 || echo "No files found"

      # Performance regression check
      - name: Performance regression check
        run: |
          # Extract key timing data for regression detection
          echo "Extracting performance metrics..."

          # Look for the latest comprehensive comparison results
          echo "Searching for comprehensive comparison files..."
          find benchmark-results -name "*.md" | sort

          LATEST_RESULT=$(find benchmark-results -name "comprehensive_comparison_*.md" | sort | tail -1)

          if [ -f "$LATEST_RESULT" ]; then
            echo "✅ Found results file: $LATEST_RESULT"
            
            # Show file content for debugging
            echo "--- File content preview ---"
            head -20 "$LATEST_RESULT"
            echo "--- End preview ---"
            
            # Extract timing data (simple regex extraction) from xlarge dataset results
            RUST_TIME=$(grep "🦀 Rust Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            GO_TIME=$(grep "🐹 Go Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            PYTHON_TIME=$(grep "🐍 Python Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            NODE_PAPAPARSE_TIME=$(grep "🟢 Node+PapaParse" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            BUN_PAPAPARSE_TIME=$(grep "🔥 Bun+PapaParse" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            
            echo "Performance Results (xlarge dataset):"
            echo "Rust: ${RUST_TIME}ms"
            echo "Go: ${GO_TIME}ms"
            echo "Python: ${PYTHON_TIME}ms" 
            echo "Node+PapaParse: ${NODE_PAPAPARSE_TIME}ms"
            echo "Bun+PapaParse: ${BUN_PAPAPARSE_TIME}ms"
            
            # Only check regressions if we have valid data
            if [ -n "$RUST_TIME" ] && [ -n "$GO_TIME" ] && [ -n "$PYTHON_TIME" ] && [ -n "$NODE_PAPAPARSE_TIME" ]; then
              # Updated regression thresholds for xlarge dataset (in milliseconds)
              RUST_THRESHOLD=50.0
              GO_THRESHOLD=250.0
              PYTHON_THRESHOLD=600.0
              NODE_PAPAPARSE_THRESHOLD=120.0
              BUN_PAPAPARSE_THRESHOLD=100.0
              
              # Check for performance regressions
              if (( $(echo "$RUST_TIME > $RUST_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Rust performance regression: ${RUST_TIME}ms > ${RUST_THRESHOLD}ms"
                echo "rust-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if (( $(echo "$GO_TIME > $GO_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Go performance regression: ${GO_TIME}ms > ${GO_THRESHOLD}ms"
                echo "go-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if (( $(echo "$PYTHON_TIME > $PYTHON_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Python performance regression: ${PYTHON_TIME}ms > ${PYTHON_THRESHOLD}ms"
                echo "python-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if (( $(echo "$NODE_PAPAPARSE_TIME > $NODE_PAPAPARSE_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Node+PapaParse performance regression: ${NODE_PAPAPARSE_TIME}ms > ${NODE_PAPAPARSE_THRESHOLD}ms"
                echo "node-papaparse-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if [ -n "$BUN_PAPAPARSE_TIME" ] && (( $(echo "$BUN_PAPAPARSE_TIME > $BUN_PAPAPARSE_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Bun+PapaParse performance regression: ${BUN_PAPAPARSE_TIME}ms > ${BUN_PAPAPARSE_THRESHOLD}ms"
                echo "bun-papaparse-regression=true" >> $GITHUB_OUTPUT
              fi
              
              # Save metrics for artifact
              echo "rust_time_ms=$RUST_TIME" > benchmark-results/performance-metrics.txt
              echo "go_time_ms=$GO_TIME" >> benchmark-results/performance-metrics.txt
              echo "python_time_ms=$PYTHON_TIME" >> benchmark-results/performance-metrics.txt
              echo "node_papaparse_time_ms=$NODE_PAPAPARSE_TIME" >> benchmark-results/performance-metrics.txt
              if [ -n "$BUN_PAPAPARSE_TIME" ]; then
                echo "bun_papaparse_time_ms=$BUN_PAPAPARSE_TIME" >> benchmark-results/performance-metrics.txt
              fi
              
              echo "✅ Performance metrics extracted and saved"
            else
              echo "⚠️ Could not extract timing data from results file"
              echo "regression-check=failed" >> $GITHUB_OUTPUT
            fi
            
          else
            echo "❌ No comprehensive comparison results found"
            echo "Available files:"
            find benchmark-results -type f | head -10
            echo "regression-check=no-file" >> $GITHUB_OUTPUT
          fi

      # Generate summary report
      - name: Generate summary report
        run: |
          cat > benchmark-results/README.md << 'EOF'
          # Performance Benchmark Results

          Generated on: $(date)
          Commit: ${{ github.sha }}
          Branch/Ref: ${{ github.ref }}
          Runner: ${{ runner.os }}

          ## Files in this artifact:

          - `benchmark-log-*.txt` - Complete benchmark execution log
          - `comprehensive_comparison_*.md` - Detailed comparison results
          - `scaling_*_*.md` - Data size scaling analysis  
          - `startup_overhead_*.md` - Startup time analysis
          - `*.json` - Machine-readable benchmark data
          - `*.csv` - Spreadsheet-compatible results
          - `performance-metrics.txt` - Key performance indicators

          ## Quick Results Summary:

          Check the `comprehensive_comparison_*.md` file for detailed results.

          ## Performance Trends:

          Monitor the `performance-metrics.txt` file for xlarge dataset (13MB, 1000 rows):
          - Rust performance should be < 50ms
          - Go performance should be < 250ms
          - Python performance should be < 600ms  
          - Node+PapaParse performance should be < 120ms
          - Bun+PapaParse performance should be < 100ms

          EOF

      # Upload benchmark results as artifacts
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/
          retention-days: 30

      # Comment on PR with benchmark results (if this is a PR)
      - name: Comment PR with benchmark summary
        if: github.event_name == 'pull_request'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find the latest comprehensive comparison MD file
            const resultsDir = 'benchmark-results';
            const files = fs.readdirSync(resultsDir);
            const compFile = files.find(f => f.startsWith('comprehensive_comparison_') && f.endsWith('.md'));

            if (compFile) {
              const tableContent = fs.readFileSync(path.join(resultsDir, compFile), 'utf8');

              if (tableContent.trim()) {
                let comment = `## 🚀 Performance Benchmark Results\n\n`;
                comment += `Commit: \`${{ github.sha }}\`\n\n`;
                comment += '### Performance Comparison\n\n';
                comment += tableContent;
                comment += `\n\n📊 **[View Full Benchmark Artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})**`;

                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              } else {
                console.log('Benchmark markdown file was found, but it is empty.');
              }
            } else {
              console.log('No comprehensive comparison markdown file found in benchmark-results.');
              console.log('Available files:', files);
            }
