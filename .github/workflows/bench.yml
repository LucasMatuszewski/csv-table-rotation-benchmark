name: Performance Benchmarks

on:
  push:
    branches: [main]
    paths-ignore:
      - "**.md"
      - "LICENSE"
      - ".gitignore"
      - ".editorconfig"
      - ".vscode/**"
  pull_request:
    branches: [main]
    paths-ignore:
      - "**.md"
      - "LICENSE"
      - ".gitignore"
      - ".editorconfig"
      - ".vscode/**"
  schedule:
    # Run benchmarks weekly on Sunday at 02:00 UTC to detect performance regressions
    - cron: "0 2 * * 0"
  workflow_dispatch:
    # Allow manual triggering

jobs:
  benchmark:
    name: Cross-Language Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
      - uses: actions/checkout@v4

      # Setup all language environments
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.21"
          cache-dependency-path: go/go.sum

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20.x"
          cache: "npm"
          cache-dependency-path: typescript/package-lock.json

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install hyperfine
        run: |
          # Install hyperfine using cargo (most reliable method)
          cargo install hyperfine --locked

      # Cache dependencies
      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            rust/target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('rust/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Cache Go dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('go/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.11-${{ hashFiles('python/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-3.11-

      # Build all implementations
      - name: Build Rust (release mode)
        run: cargo build --release
        working-directory: rust

      - name: Build Go (release mode)
        run: go build -ldflags="-s -w" -o ./bin/rotate ./cmd/rotate
        working-directory: go

      - name: Build TypeScript
        run: |
          npm ci
          npm run build
        working-directory: typescript

      - name: Install Python package
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
        working-directory: python

      # Verify all binaries work
      - name: Verify implementations
        run: |
          echo "Testing Rust implementation..."
          ./rust/target/release/rotate_cli input-samples/sample-1k.csv | head -5

          echo "Testing Go implementation..."
          ./go/bin/rotate input-samples/sample-1k.csv | head -5

          echo "Testing TypeScript implementation..."
          node typescript/dist/index.js input-samples/sample-1k.csv | head -5

          echo "Testing Python implementation..."
          python -m rotate_cli input-samples/sample-1k.csv | head -5

      # Create benchmark results directory
      - name: Create benchmark results directory
        run: |
          mkdir -p benchmark-results
          date > benchmark-results/benchmark-info.txt
          echo "Runner: ${{ runner.os }}" >> benchmark-results/benchmark-info.txt
          echo "Commit: ${{ github.sha }}" >> benchmark-results/benchmark-info.txt
          echo "Ref: ${{ github.ref }}" >> benchmark-results/benchmark-info.txt

      # Run comprehensive benchmarks
      - name: Run comprehensive benchmarks
        run: |
          # Make script executable
          chmod +x ./benchmarks/run_hyperfine.sh

          # Run benchmarks with timestamp
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          if ! ./benchmarks/run_hyperfine.sh 2>&1 | tee benchmark-results/benchmark-log-${TIMESTAMP}.txt; then
            echo "⚠️ Benchmark script encountered issues, but continuing with available results..."
            echo "script-failed=true" >> $GITHUB_OUTPUT
          fi

          # Copy all generated results
          if [ -d "./benchmarks/results" ]; then
            cp -r ./benchmarks/results/* benchmark-results/ || true
          fi

          # Debug: List what files were actually generated
          echo "Generated files in benchmarks/results:"
          find ./benchmarks/results -name "*.md" -o -name "*.json" -o -name "*.csv" | head -10 || echo "No files found"

          echo "Files copied to benchmark-results:"
          find benchmark-results -name "*.md" -o -name "*.json" -o -name "*.csv" | head -10 || echo "No files found"

      # Performance regression check
      - name: Performance regression check
        run: |
          # Extract key timing data for regression detection
          echo "Extracting performance metrics..."

          # Look for the latest comprehensive comparison results
          echo "Searching for comprehensive comparison files..."
          find benchmark-results -name "*.md" | sort

          LATEST_RESULT=$(find benchmark-results -name "comprehensive_comparison_*.md" | sort | tail -1)

          if [ -f "$LATEST_RESULT" ]; then
            echo "✅ Found results file: $LATEST_RESULT"
            
            # Show file content for debugging
            echo "--- File content preview ---"
            head -20 "$LATEST_RESULT"
            echo "--- End preview ---"
            
            # Extract timing data (simple regex extraction)
            RUST_TIME=$(grep "🦀 Rust Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            GO_TIME=$(grep "🐹 Go Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            PYTHON_TIME=$(grep "🐍 Python Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            TS_TIME=$(grep "📜 TypeScript Implementation" "$LATEST_RESULT" | grep -oP '\d+\.\d+' | head -1)
            
            echo "Performance Results:"
            echo "Rust: ${RUST_TIME}ms"
            echo "Go: ${GO_TIME}ms"
            echo "Python: ${PYTHON_TIME}ms" 
            echo "TypeScript: ${TS_TIME}ms"
            
            # Only check regressions if we have valid data
            if [ -n "$RUST_TIME" ] && [ -n "$GO_TIME" ] && [ -n "$PYTHON_TIME" ] && [ -n "$TS_TIME" ]; then
              # Basic regression thresholds (in milliseconds)
              RUST_THRESHOLD=10.0
              GO_THRESHOLD=25.0
              PYTHON_THRESHOLD=100.0
              TS_THRESHOLD=100.0
              
              # Check for performance regressions
              if (( $(echo "$RUST_TIME > $RUST_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Rust performance regression: ${RUST_TIME}ms > ${RUST_THRESHOLD}ms"
                echo "rust-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if (( $(echo "$GO_TIME > $GO_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Go performance regression: ${GO_TIME}ms > ${GO_THRESHOLD}ms"
                echo "go-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if (( $(echo "$PYTHON_TIME > $PYTHON_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential Python performance regression: ${PYTHON_TIME}ms > ${PYTHON_THRESHOLD}ms"
                echo "python-regression=true" >> $GITHUB_OUTPUT
              fi
              
              if (( $(echo "$TS_TIME > $TS_THRESHOLD" | bc -l) )); then
                echo "⚠️ Potential TypeScript performance regression: ${TS_TIME}ms > ${TS_THRESHOLD}ms"
                echo "ts-regression=true" >> $GITHUB_OUTPUT
              fi
              
              # Save metrics for artifact
              echo "rust_time_ms=$RUST_TIME" > benchmark-results/performance-metrics.txt
              echo "go_time_ms=$GO_TIME" >> benchmark-results/performance-metrics.txt
              echo "python_time_ms=$PYTHON_TIME" >> benchmark-results/performance-metrics.txt
              echo "typescript_time_ms=$TS_TIME" >> benchmark-results/performance-metrics.txt
              
              echo "✅ Performance metrics extracted and saved"
            else
              echo "⚠️ Could not extract timing data from results file"
              echo "regression-check=failed" >> $GITHUB_OUTPUT
            fi
            
          else
            echo "❌ No comprehensive comparison results found"
            echo "Available files:"
            find benchmark-results -type f | head -10
            echo "regression-check=no-file" >> $GITHUB_OUTPUT
          fi

      # Generate summary report
      - name: Generate summary report
        run: |
          cat > benchmark-results/README.md << 'EOF'
          # Performance Benchmark Results

          Generated on: $(date)
          Commit: ${{ github.sha }}
          Branch/Ref: ${{ github.ref }}
          Runner: ${{ runner.os }}

          ## Files in this artifact:

          - `benchmark-log-*.txt` - Complete benchmark execution log
          - `comprehensive_comparison_*.md` - Detailed comparison results
          - `scaling_*_*.md` - Data size scaling analysis  
          - `startup_overhead_*.md` - Startup time analysis
          - `*.json` - Machine-readable benchmark data
          - `*.csv` - Spreadsheet-compatible results
          - `performance-metrics.txt` - Key performance indicators

          ## Quick Results Summary:

          Check the `comprehensive_comparison_*.md` file for detailed results.

          ## Performance Trends:

          Monitor the `performance-metrics.txt` file for:
          - Rust performance should be < 10ms
          - Go performance should be < 25ms
          - Python performance should be < 100ms  
          - TypeScript performance should be < 100ms

          EOF

      # Upload benchmark results as artifacts
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/
          retention-days: 30

      # Comment on PR with benchmark results (if this is a PR)
      - name: Comment PR with benchmark summary
        if: github.event_name == 'pull_request'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find the latest comprehensive comparison file
            const resultsDir = 'benchmark-results';
            const files = fs.readdirSync(resultsDir);
            const compFile = files.find(f => f.startsWith('comprehensive_comparison_'));

            if (compFile) {
              const content = fs.readFileSync(path.join(resultsDir, compFile), 'utf8');
              
              console.log('File content debug:');
              console.log('---');
              console.log(content);
              console.log('---');
              
              // Build the comment with the raw table content
              let comment = `## 🚀 Performance Benchmark Results\n\n`;
              comment += `Commit: \`${{ github.sha }}\`\n\n`;
              
              // Since the file is small and mostly just the table, include it directly
              if (content.trim()) {
                comment += `### Performance Comparison\n\n`;
                comment += content.trim() + '\n\n';
                
                // Extract key metrics for summary
                const lines = content.split('\n');
                const rustLine = lines.find(line => line.includes('🦀 Rust Implementation'));
                const goLine = lines.find(line => line.includes('🐹 Go Implementation'));
                const tsLine = lines.find(line => line.includes('📜 TypeScript Implementation'));
                const pythonLine = lines.find(line => line.includes('🐍 Python Implementation'));
                
                if (rustLine && goLine && tsLine && pythonLine) {
                  comment += `### Quick Summary\n\n`;
                  
                  // Extract times (simplified - just grab the first number after the pipe)
                  const extractTime = (line) => {
                    const match = line.match(/\|\s*(\d+\.?\d*)\s*[±]?\s*[\d.]*\s*\|/);
                    return match ? parseFloat(match[1]) : 0;
                  };
                  
                  const rustTime = extractTime(rustLine);
                  const goTime = extractTime(goLine);
                  const tsTime = extractTime(tsLine);
                  const pythonTime = extractTime(pythonLine);
                  
                  if (rustTime > 0) {
                    comment += `- 🦀 **Rust**: ${rustTime}ms (baseline)\n`;
                    if (goTime > 0) comment += `- 🐹 **Go**: ${goTime}ms (${(goTime/rustTime).toFixed(1)}× slower)\n`;
                    if (pythonTime > 0) comment += `- 🐍 **Python**: ${pythonTime}ms (${(pythonTime/rustTime).toFixed(1)}× slower)\n`;
                    if (tsTime > 0) comment += `- 📜 **TypeScript**: ${tsTime}ms (${(tsTime/rustTime).toFixed(1)}× slower)\n`;
                    comment += '\n';
                  }
                }
              } else {
                comment += `⚠️ No benchmark data found in results file.\n\n`;
              }
              
              comment += `📊 **[View Full Benchmark Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})**\n\n`;
              comment += `> *Benchmarks run on: \`${{ runner.os }}\` • Generated: ${new Date().toISOString().split('T')[0]}*`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } else {
              console.log('No comprehensive comparison file found');
              console.log('Available files:', fs.readdirSync(resultsDir));
            }
